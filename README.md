# Multi-Instance Transfer Learning on TCR LLMs for Cancer Prediction

This project aims to investigate difference between the expressivity of physico-chemical properties (i.e. Atchley factors) and language models in cancer classifications using TCR CDR3 sequences.  With the use of a language model, we obtained high AUCs in classifying whether a patient has cancer.

For more details regarding this research, please view my dissertation [here](manuscript.pdf).

---

## Installation

1. Download this repository
2. Create a Python Environment venv through
   ``python3 -m venv $YOUR-VENV-NAME-HERE$``
3. Activate your virtual environment, and run the following command
   ``python -m pip install -r scripts/requirements.txt``
   if your computer is a Windows Computer, and 
   ``python -m pip install -r scripts/requirements-linux.txt``
   if it is Linux Ubuntu instead.

> [!NOTE]
> You should install your own version of PyTorch depending on your CUDA version before installing the `requirements.txt`.  You may find instructions of installing PyTorch [here](https://pytorch.org/).

> [!NOTE]
> SCEPTR is closed source as of this current moment.  Please contact [this email](mailto://rcwyuen@gmail.com) for more details.

---

To download data for this project, please follow the instructions below.

### Downloading Required Files

You would need to have access to the Chain Lab RDS to download the data used in this study.  You should also be connected to UCL's WiFi or UCL's VPN to download the files.

#### Data Fetching

To pull the data from the Chain Lab RDS, you may run the following command.

```
python loaders/load_cdr.py -config_path loaders/config.json
```

Please modify ``rds_mountpoint`` in ``loaders/config.json`` to your mountpoint in your computer.  You should not amend other configurations in the file.

To compress the data (i.e. removing all data other than V call, J call and CDR3 sequences), you may run

```
python utils/file-compressor.py
```

#### Pre-Trained Model Fetching

To download the [TCR-BERT](https://www.biorxiv.org/content/10.1101/2021.11.18.469186v1) pretrained model, you may run the following command:

```
python loaders/load_ptm.py -o model
```

This will invoke a script to download both variants of TCR-BERT from [HuggingFace](https://huggingface.co/).

## Usage

The training script takes in a configuration file, which can be generated by:
``python trainer.py --make``
This will continue the execution with the default configurations.  If you want it to end after generating the script you may run:
``python trainer.py --make --end``

### Training Configurations

To modify the training configurations, you may modify the config.json as generated.  The configurations for the 3 training scripts are different.  You may find the description for each field in each training script as below:

| Encoding Method  | Requirements Filename        |
| ---------------- | ---------------------------- |
| SCEPTR           | [Descriptions Here](instructions/sceptr-config.md) |
| TCR-BERT         | [Descriptions Here](instructions/tcrbert-config.md) |
| Symbolic         | [Descriptions Here](instructions/symbolic-config.md) |


### Post-Training Files & Checkpoints

Throughout training, checkpoints will be made alongside with this current epoch's training statistics such as loss, accuracies and sufficient data to compute the AUC.  This repository provides Jupyter Files to analyse the whole training loop's statistics.  The Jupyter Files are as follows:

#### `demo.ipynb`

Jupyter Notebook used in the Demo Video.  Able to retrieve the training and testing dataset and inferences on them again.  It can also visualise the non-zero weights on the TCRs with their Amino Acid Sequencing.

#### `specs-for-model.ipynb`

Generates the Confusion Matrix and a graph of amount of TCRs against Loss.

#### `training-stats-analysis.ipynb`

Generates a report on the model's loss, accuracy and AUC throughout its training loop.

#### `vector-similarity.ipynb`

Compares between two different model's parameters to obtain the similarity between them in terms of a dot product.

#### `best-result-report.ipynb`

Covers the following:

- `vector-similarity.ipynb`'s graph to see how the scoring and classifying layers have converged through time.
- The whole of `training-stats-analysis.ipynb`, with labels on the best epoch chosen by highest testing AUC (as the vertical lines on graphs)
- The whole of `specs-for-model` on the best epoch chosen by highest AUC.

## Known Errors

- Path Length Problems: If your path is too long in Windows, you are prone to the following error:

  ``DLL load failed while importing $SOMETHING$: The filename or extension is too long.``

  A mitigation strategy is to use the global Python, or to put your files in a shorter directory.
