# Multi-Instance Transfer Learning on TCR-BERT for Cancer Prediction

This project aims to apply transfer learning to [TCR-BERT](https://www.biorxiv.org/content/10.1101/2021.11.18.469186v1) to classify Cancer using a Multi-Instance approach to TCR Repertoires.

For more details regarding this research, please view my dissertation [here](https://google.com).

## Installation

1. Clone this repository
2. Create a Python Environment venv through
   ``python3 -m venv $YOUR-VENV-NAME-HERE``
3. Activate your virtual environment, and run the following command
   ``python -m pip install -r scripts/requirements.txt``
   Depending on your Operating System and CUDA requirements, please change ``requirements.txt`` to the following appropriately:

| Operating System | CUDA  | Requirements Filename        |
| ---------------- | ----- | ---------------------------- |
| Windows          | True  | ``requirements.txt``         |
| Windows          | False | ``requirements-cpuonly.txt`` |
| Linux            | True  | ``requirements-linux.txt``   |
| Linux            | False | ``requirements-linux.txt``   |

4. To download all required files, please follow the subsection below.

### Downloading Required Files

To download required files, it is assume that you have access to the Chain Lab RDS, and you are connected to UCL's WiFi or UCL's VPN.

#### Data Fetching

The TRACERx dataset has been used for this task, and it is assumed that you will have access to the Chain Lab RDS.  To pull the data from the Chain Lab RDS, you may run the following command.  Please make sure that you are either using UCL's WiFi or connected to UCL VPN.

```
python loaders/load_cdr.py -config_path loaders/config.json
```

Please modify ``rds_mountpoint`` in ``config.json`` to your mountpoint in your computer.  You may leave others as is.

To compress the data (i.e. removing all data other than CDR1, CDR2 and CDR3 sequences), you may run

```
python utils/file-compressor.py
```

where this will generate the folder ``compressed`` under ``data``.

#### Pre-Trained Model Fetching

To download the [TCR-BERT](https://www.biorxiv.org/content/10.1101/2021.11.18.469186v1) pretrained model, you may run the following command:

```
python loaders/load_ptm.py -o model
```

This will invoke a script to download both [TCR-BERT for Classification across Antigen Labels from PIRD](https://huggingface.co/wukevin/tcr-bert) and [Masked Language Modelling TCR-BERT](https://huggingface.co/wukevin/tcr-bert-mlm-only) from [HuggingFace](https://huggingface.co/).

## Usage

The training script takes in a configuration file, which can be generated by:
``python trainer.py --make``
This will continue the execution with the default setting.  If you want it to end after generating the script you may run:
``python trainer.py --make --end``

### Training Configurations

To modify the training configurations, you may modify the config.json as generated.  The descriptions to the fields are as follows.

| Field                    | Expected Datatype            | Descriptions                                                                                                                                   |
| ------------------------ | ---------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------- |
| `input-path`           | `str`                      | The path to the Dataset                                                                                                                        |
| `output-path`          | `str`                      | Location to flush all outputs                                                                                                                  |
| `model-path`           | `str`                      | The path to the Model                                                                                                                          |
| `maa-model`            | `bool`                     | Whether to use the Masked Amino Acid Model, for details, see[TCR-BERT](https://www.biorxiv.org/content/10.1101/2021.11.18.469186v1)'s white paper |
| `negative-dir`         | `list[str]`                | Directory Location to Control Data                                                                                                             |
| `positive-dir`         | `list[str]`                | Directory Location to Cancer Patients                                                                                                          |
| `cdr1`                 | `bool`                     | Whether to include CDR1 sequences into classification.  For TCR-BERT, this should be set to `false`                                          |
| `cdr2`                 | `bool`                     | Whether to include CDR2 sequences into classification.  For TCR-BERT, this should be set to `false`                                          |
| `batch-size`           | `int`                      | Amount of TCR sequences inside each patient to pass into the model at once                                                                     |
| `epoch`                | `int`                      | Amount of Epochs to train the model                                                                                                            |
| `lr`                   | `float` or `list[float]` | Learning Rate, or a List of Learning Rates                                                                                                     |
| `change-lr-at`         | `float` or `list[float]` | Epochs to change Learning Rate at.  This should be the same datatype as "lr"                                                                   |
| `train-split`          | `float`                    | The proportion of data to be served as the training data                                                                                       |
| `bag-accummulate-loss` | `int`                      | The amount of patients to incur a step down the gradient                                                                                       |
| `l2-penalty`           | `float`                    | Amount of Weight Decay                                                                                                                         |

### Post-Training Files & Checkpoints

Throughout training, checkpoints will be made alongside with this current epoch's training statistics such as loss, accuracies and sufficient data to compute the AUC.  This repository provides Jupyter Files to analyse the whole training loop's statistics.  The Jupyter Files are as follows:

#### `demo.ipynb`
Jupyter Notebook used in the Demo Video.  Able to retrieve the training and testing dataset and inferences on them again.  It can also visualise the non-zero weights on the TCRs with their Amino Acid Sequencing.

#### `specs-for-model.ipynb`
Generates the Confusion Matrix and a graph of amount of TCRs against Loss.

#### `training-stats-analysis.ipynb`
Generates a report on the model's loss, accuracy and AUC throughout its training loop.

#### `vector-similarity.ipynb`
Compares between two different model's parameters to obtain the similarity between them in terms of a dot product.

#### `best-result-report.ipynb`
Covers the following: 
- `vector-similarity.ipynb`'s graph to see how the scoring and classifying layers have converged through time.
- The whole of `training-stats-analysis.ipynb`, with labels on the best epoch chosen by highest testing AUC (as the vertical lines on graphs)
- The whole of `specs-for-model` on the best epoch chosen by highest AUC.

## Known Errors

- Path Length Problems: If your path is too long in Windows, you are prone to the following error:

  ``DLL load failed while importing $SOMETHING$: The filename or extension is too long.``

  A mitigation strategy is to use the global Python, or to put your files in a shorter directory.
