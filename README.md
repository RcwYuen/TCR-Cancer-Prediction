# Multi-Instance Transfer Learning on TCR LLMs for Cancer Prediction

This project aims to investigate difference between the expressivity of physico-chemical properties (i.e. Atchley factors) and language models in cancer classifications using TCR CDR3 sequences.  With the use of a language model, we obtained high AUCs in classifying whether a patient has cancer.

For more details regarding this research, please view my dissertation [here](manuscript.pdf).

---

## Installation

1. Download this repository
2. Create a Python Environment venv through
   ``python3 -m venv $YOUR-VENV-NAME-HERE$``
3. Activate your virtual environment, and run the following command
   ``python -m pip install -r scripts/requirements.txt``
   if your computer is a Windows Computer, and 
   ``python -m pip install -r scripts/requirements-linux.txt``
   if it is Linux Ubuntu instead.

> [!NOTE]
> You should install your own version of PyTorch depending on your CUDA version before installing the `requirements.txt`.  You may find instructions in installing PyTorch [here](https://pytorch.org/).

> [!NOTE]
> SCEPTR is closed source.  Please install SCEPTR as of its instructions if you are granted permission to SCEPTR.  Please contact me [here](mailto://rcwyuen@gmail.com) for more details.

---

To download all required files, please follow the subsection below.

### Downloading Required Files

To download required files, it is assume that you have access to the Chain Lab RDS, and you are connected to UCL's WiFi or UCL's VPN.

#### Data Fetching

The TRACERx dataset has been used for this task, and it is assumed that you will have access to the Chain Lab RDS.  To pull the data from the Chain Lab RDS, you may run the following command.  Please make sure that you are either using UCL's WiFi or connected to UCL VPN.

```
python loaders/load_cdr.py -config_path loaders/config.json
```

Please modify ``rds_mountpoint`` in ``config.json`` to your mountpoint in your computer.  You may leave others as is.

To compress the data (i.e. removing all data other than CDR1, CDR2 and CDR3 sequences), you may run

```
python utils/file-compressor.py
```

where this will generate the folder ``compressed`` under ``data``.

#### Pre-Trained Model Fetching

To download the [TCR-BERT](https://www.biorxiv.org/content/10.1101/2021.11.18.469186v1) pretrained model, you may run the following command:

```
python loaders/load_ptm.py -o model
```

This will invoke a script to download both [TCR-BERT for Classification across Antigen Labels from PIRD](https://huggingface.co/wukevin/tcr-bert) and [Masked Language Modelling TCR-BERT](https://huggingface.co/wukevin/tcr-bert-mlm-only) from [HuggingFace](https://huggingface.co/).

## Usage

The training script takes in a configuration file, which can be generated by:
``python trainer.py --make``
This will continue the execution with the default setting.  If you want it to end after generating the script you may run:
``python trainer.py --make --end``

### Training Configurations

To modify the training configurations, you may modify the config.json as generated.  The configurations for the 3 training scripts are different.  You may find the description for each field in each training script as below:

| Encoding Method  | Requirements Filename        |
| ---------------- | ---------------------------- |
| SCEPTR           | [Descriptions Here](instructions/sceptr-config.md) |
| TCR-BERT         | [Descriptions Here](instructions/tcrbert-config.md) |
| Symbolic         | [Descriptions Here](instructions/symbolic-config.md) |


### Post-Training Files & Checkpoints

Throughout training, checkpoints will be made alongside with this current epoch's training statistics such as loss, accuracies and sufficient data to compute the AUC.  This repository provides Jupyter Files to analyse the whole training loop's statistics.  The Jupyter Files are as follows:

#### `demo.ipynb`

Jupyter Notebook used in the Demo Video.  Able to retrieve the training and testing dataset and inferences on them again.  It can also visualise the non-zero weights on the TCRs with their Amino Acid Sequencing.

#### `specs-for-model.ipynb`

Generates the Confusion Matrix and a graph of amount of TCRs against Loss.

#### `training-stats-analysis.ipynb`

Generates a report on the model's loss, accuracy and AUC throughout its training loop.

#### `vector-similarity.ipynb`

Compares between two different model's parameters to obtain the similarity between them in terms of a dot product.

#### `best-result-report.ipynb`

Covers the following:

- `vector-similarity.ipynb`'s graph to see how the scoring and classifying layers have converged through time.
- The whole of `training-stats-analysis.ipynb`, with labels on the best epoch chosen by highest testing AUC (as the vertical lines on graphs)
- The whole of `specs-for-model` on the best epoch chosen by highest AUC.

## Known Errors

- Path Length Problems: If your path is too long in Windows, you are prone to the following error:

  ``DLL load failed while importing $SOMETHING$: The filename or extension is too long.``

  A mitigation strategy is to use the global Python, or to put your files in a shorter directory.
